{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests de Modèles - House Prices Prediction\n",
    "## Laplace Immo - Projet Data Science\n",
    "\n",
    "Ce notebook compare différents algorithmes de Machine Learning pour prédire les prix des maisons.\n",
    "Objectif: Identifier le meilleur modèle pour la prédiction des prix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Chargement des données\n",
    "train_df = pd.read_csv('../data/raw/train.csv')\n",
    "print(f\"Donn\u00e9es charg\u00e9es: {train_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prétraitement des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== PR\u00c9TRAITEMENT DES DONN\u00c9ES ===\")\n",
    "\n",
    "# Ing\u00e9nierie des features\n",
    "train_df['HouseAge'] = train_df['YrSold'] - train_df['YearBuilt']\n",
    "train_df['TotalSF'] = train_df['GrLivArea'] + train_df['TotalBsmtSF'].fillna(0)\n",
    "train_df['OverallScore'] = train_df['OverallQual'] * train_df['OverallCond']\n",
    "\n",
    "# S\u00e9paration features/target\n",
    "X = train_df.drop(['SalePrice', 'Id'], axis=1)\n",
    "y = train_df['SalePrice']\n",
    "\n",
    "# Identification des types de variables\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Features num\u00e9riques: {len(numerical_features)}\")\n",
    "print(f\"Features cat\u00e9gorielles: {len(categorical_features)}\")\n",
    "\n",
    "# Gestion des valeurs manquantes\n",
    "for col in numerical_features:\n",
    "    X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "for col in categorical_features:\n",
    "    X[col] = X[col].fillna('None')\n",
    "\n",
    "print(\"Valeurs manquantes trait\u00e9es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipeline de Prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== PIPELINE DE PR\u00c9TRAITEMENT ===\")\n",
    "\n",
    "# Pipeline pour les variables num\u00e9riques\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline pour les variables cat\u00e9gorielles\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine les pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Division des donn\u00e9es\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Jeu d'entra\u00eenement: {X_train.shape}\")\n",
    "print(f\"Jeu de test: {X_test.shape}\")\n",
    "\n",
    "# Application du pr\u00e9traitement\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "print(f\"Donn\u00e9es pr\u00e9trait\u00e9es: {X_train_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tests des Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TESTS DES MOD\u00c8LES ===\")\n",
    "\n",
    "# Dictionnaire des mod\u00e8les \u00e0 tester\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=1.0),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Fonction d'\u00e9valuation\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Entra\u00eene et \u00e9value un mod\u00e8le\"\"\"\n",
    "    # Entra\u00eenement\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Pr\u00e9dictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # M\u00e9triques\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "    \n",
    "    return {\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'cv_rmse': cv_rmse\n",
    "    }\n",
    "\n",
    "# \u00c9valuation de tous les mod\u00e8les\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n\u00c9valuation de {name}...\")\n",
    "    results[name] = evaluate_model(model, X_train_processed, y_train, X_test_processed, y_test)\n",
    "    print(f\"  RMSE Test: {results[name]['test_rmse']:,.0f}\")\n",
    "    print(f\"  R\u00b2 Test: {results[name]['test_r2']:.3f}\")\n",
    "    print(f\"  CV RMSE: {results[name]['cv_rmse']:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualisation des Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== VISUALISATION DES R\u00c9SULTATS ===\")\n",
    "\n",
    "# Cr\u00e9ation du DataFrame de r\u00e9sultats\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nTableau comparatif des mod\u00e8les:\")\n",
    "print(results_df.round(3))\n",
    "\n",
    "# Graphique comparatif\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# RMSE Test\n",
    "axes[0, 0].bar(results_df.index, results_df['test_rmse'], color='skyblue', alpha=0.7)\n",
    "axes[0, 0].set_title('RMSE Test par Mod\u00e8le')\n",
    "axes[0, 0].set_ylabel('RMSE ($)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# R\u00b2 Test\n",
    "axes[0, 1].bar(results_df.index, results_df['test_r2'], color='lightgreen', alpha=0.7)\n",
    "axes[0, 1].set_title('R\u00b2 Test par Mod\u00e8le')\n",
    "axes[0, 1].set_ylabel('R\u00b2 Score')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE Test\n",
    "axes[1, 0].bar(results_df.index, results_df['test_mae'], color='salmon', alpha=0.7)\n",
    "axes[1, 0].set_title('MAE Test par Mod\u00e8le')\n",
    "axes[1, 0].set_ylabel('MAE ($)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# CV RMSE\n",
    "axes[1, 1].bar(results_df.index, results_df['cv_rmse'], color='gold', alpha=0.7)\n",
    "axes[1, 1].set_title('CV RMSE par Mod\u00e8le')\n",
    "axes[1, 1].set_ylabel('CV RMSE ($)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Identification du Meilleur Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification du meilleur mod\u00e8le\n",
    "best_model_name = results_df['test_r2'].idxmax()\n",
    "best_r2 = results_df.loc[best_model_name, 'test_r2']\n",
    "best_rmse = results_df.loc[best_model_name, 'test_rmse']\n",
    "best_cv_rmse = results_df.loc[best_model_name, 'cv_rmse']\n",
    "\n",
    "print(f\"\\n\ud83c\udfc6 MEILLEUR MOD\u00c8LE: {best_model_name}\")\n",
    "print(f\"   R\u00b2 Score: {best_r2:.3f}\")\n",
    "print(f\"   RMSE: ${best_rmse:,.0f}\")\n",
    "print(f\"   CV RMSE: ${best_cv_rmse:,.0f}\")\n",
    "\n",
    "# Analyse des r\u00e9sidus du meilleur mod\u00e8le\n",
    "best_model = models[best_model_name]\n",
    "y_pred_test = best_model.predict(X_test_processed)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# R\u00e9sidus vs Valeurs pr\u00e9dites\n",
    "plt.subplot(1, 2, 1)\n",
    "residuals = y_test - y_pred_test\n",
    "plt.scatter(y_pred_test, residuals, alpha=0.5, color='steelblue')\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel('Valeurs pr\u00e9dites')\n",
    "plt.ylabel('R\u00e9sidus')\n",
    "plt.title(f'R\u00e9sidus - {best_model_name}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution des r\u00e9sidus\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(residuals, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "plt.xlabel('R\u00e9sidus')\n",
    "plt.ylabel('Fr\u00e9quence')\n",
    "plt.title(f'Distribution des r\u00e9sidus - {best_model_name}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/best_model_residuals.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u00c9tude des r\u00e9sidus:\")\n",
    "print(f\"Moyenne des r\u00e9sidus: {residuals.mean():.0f}\")\n",
    "print(f\"\u00c9cart-type des r\u00e9sidus: {residuals.std():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimisation du Meilleur Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== OPTIMISATION DU MEILLEUR MOD\u00c8LE ===\")\n",
    "\n",
    "# Optimisation des hyperparam\u00e8tres pour Gradient Boosting\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "print(\"Recherche des meilleurs hyperparam\u00e8tres...\")\n",
    "grid_search = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_processed, y_train)\n",
    "\n",
    "print(f\"Meilleurs param\u00e8tres: {grid_search.best_params_}\")\n",
    "print(f\"Meilleur score CV: {-grid_search.best_score_:,.0f}\")\n",
    "\n",
    "# \u00c9valuation du mod\u00e8le optimis\u00e9\n",
    "best_model_optimized = grid_search.best_estimator_\n",
    "y_pred_optimized = best_model_optimized.predict(X_test_processed)\n",
    "\n",
    "optimized_rmse = np.sqrt(mean_squared_error(y_test, y_pred_optimized))\n",
    "optimized_r2 = r2_score(y_test, y_pred_optimized)\n",
    "optimized_mae = mean_absolute_error(y_test, y_pred_optimized)\n",
    "\n",
    "print(f\"\\nR\u00e9sultats du mod\u00e8le optimis\u00e9:\")\n",
    "print(f\"RMSE: ${optimized_rmse:,.0f}\")\n",
    "print(f\"R\u00b2: {optimized_r2:.3f}\")\n",
    "print(f\"MAE: ${optimized_mae:,.0f}\")\n",
    "\n",
    "# Comparaison avant/apr\u00e8s optimisation\n",
    "print(f\"\\nAm\u00e9lioration:\")\n",
    "print(f\"R\u00b2: {optimized_r2 - best_r2:.3f} points de gain\")\n",
    "print(f\"RMSE: ${best_rmse - optimized_rmse:,.0f} de r\u00e9duction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sauvegarde du Modèle Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "print(\"=== SAUVEGARDE DU MOD\u00c8LE FINAL ===\")\n",
    "\n",
    "# Sauvegarde du mod\u00e8le et du pr\u00e9processeur\n",
    "model_final = {\n",
    "    'model': best_model_optimized,\n",
    "    'preprocessor': preprocessor,\n",
    "    'model_name': best_model_name,\n",
    "    'metrics': {\n",
    "        'rmse': optimized_rmse,\n",
    "        'r2': optimized_r2,\n",
    "        'mae': optimized_mae\n",
    "    }\n",
    "}\n",
    "\n",
    "# Cr\u00e9er le r\u00e9pertoire des mod\u00e8les s'il n'existe pas\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Sauvegarde\n",
    "joblib.dump(model_final, '../models/house_prices_model.pkl')\n",
    "print(\"Mod\u00e8le final sauvegard\u00e9 dans: ../models/house_prices_model.pkl\")\n",
    "\n",
    "# Sauvegarde des r\u00e9sultats\n",
    "results_df.to_csv('../reports/model_comparison_results.csv')\n",
    "print(\"R\u00e9sultats sauvegard\u00e9s dans: ../reports/model_comparison_results.csv\")\n",
    "\n",
    "# Sauvegarde des meilleurs param\u00e8tres\n",
    "with open('../reports/best_params.txt', 'w') as f:\n",
    "    f.write(f\"Meilleur mod\u00e8le: {best_model_name}\\n\")\n",
    "    f.write(f\"Meilleurs param\u00e8tres: {grid_search.best_params_}\\n\")\n",
    "    f.write(f\"R\u00b2 Score: {optimized_r2:.3f}\\n\")\n",
    "    f.write(f\"RMSE: ${optimized_rmse:,.0f}\\n\")\n",
    "    f.write(f\"MAE: ${optimized_mae:,.0f}\\n\")\n",
    "\n",
    "print(\"Param\u00e8tres sauvegard\u00e9s dans: ../reports/best_params.txt\")\n",
    "\n",
    "# R\u00e9sum\u00e9 final\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"R\u00c9SUM\u00c9 FINAL DU PROJET HOUSE PRICES PREDICTION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mod\u00e8le retenu: {best_model_name}\")\n",
    "print(f\"Performance R\u00b2: {optimized_r2:.1%}\")\n",
    "print(f\"Erreur moyenne (RMSE): ${optimized_rmse:,.0f}\")\n",
    "print(f\"Erreur absolue moyenne (MAE): ${optimized_mae:,.0f}\")\n",
    "print(\"\\nLe mod\u00e8le est pr\u00eat pour le d\u00e9ploiement en production!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}