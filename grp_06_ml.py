# -*- coding: utf-8 -*-
"""Grp_06_ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10EPSi11sQ8-lKZrDRxFokw13BTF8f0f0

#**Projet 2 de Machine Learning : "prédiction du prix des maisons"**

**Description:**

description du projet

##**Importation des librairies**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
import seaborn as sns
from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.decomposition import PCA
from sklearn.feature_selection import mutual_info_regression, SelectKBest
from scipy.stats import skew, boxcox_normmax
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.api import add_constant
from scipy.special import boxcox1p
import warnings
warnings.filterwarnings('ignore')
from sklearn.metrics import mean_squared_error, r2_score


# Configuration
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 11

# Commented out IPython magic to ensure Python compatibility.
# Installation de ydata-profiling si ce n'est pas déjà fait
try:
    from ydata_profiling import ProfileReport
except ImportError:
#     %pip install ydata-profiling
    from ydata_profiling import ProfileReport

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

"""##**1. Analyse exploratoire des données**"""

df_train = pd.read_csv('/content/train.csv')

"""###**1.1. Informations générales**

*Interprétation des résultats de cette partie*
"""

# Dimensions des deux jeux de données
print(df_train.shape)

df_train.head(5)

df_train.info()

# stats sur les valeurs manquantes
na_values = pd.DataFrame(df_train.isna().sum(), columns=['na_numbers'])
na_values['na_percentage'] = na_values['na_numbers'] / len(df_train)
na_values.sort_values(by='na_percentage', ascending=False, inplace=True)
na_values

# Rapport de profilage pour df_train
profile = ProfileReport(df_train, title="Rapport de Profilage du Dataset d'Entraînement")
profile

# Séparation des variables en fonction de leur type
df_numericals = df_train.select_dtypes(include=np.number)
df_numericals.drop(columns=['Id','SalePrice'], inplace=True)

# Retirer aussi OverallQual et OverallCond qui sont numeriques, mais
# doivent être traitées comme categorielles ordianles
df_numericals.drop(columns=['OverallQual','OverallCond'], inplace=True)

df_numericals.shape

df_numericals.columns

df_numericals.head(3)

# Variables categorielles ordinales
ordinal_features = [
    "OverallQual",      # déjà numérique mais ORDINALE
    "OverallCond",      # déjà numérique mais ORDINALE

    "ExterQual",
    "ExterCond",

    "BsmtQual",
    "BsmtCond",
    "BsmtExposure",
    "BsmtFinType1",
    "BsmtFinType2",

    "HeatingQC",

    "KitchenQual",

    "Functional",

    "FireplaceQu",

    "GarageFinish",
    "GarageQual",
    "GarageCond",

    "PoolQC",

    "Fence"
]

df_ordinals = df_train[ordinal_features]
df_ordinals.shape

df_ordinals.head(3)

# Variables categorielles nominales
nominal_features = df_train.columns.difference(df_numericals.columns).difference(df_ordinals.columns)
nominal_features = nominal_features.drop('Id')
nominal_features = nominal_features.drop('SalePrice')
nominal_features

df_nominals = df_train[nominal_features]
df_nominals.shape

df_nominals.head(3)

# Vérification du nombre total de colonnes
print(df_numericals.shape[1] + df_ordinals.shape[1] + df_nominals.shape[1])

# Séparation des variables
numeric_features = df_numericals.columns.tolist()

categorical_features = df_train.select_dtypes(include=['object']).columns.tolist()

# Vraies catégorielles (non ordinales)
true_categorical = nominal_features.tolist()


print(f"\ Répartition des variables:")
print(f"  • Variables numériques continues: {len(numeric_features)}")
print(f"  • Variables catégorielles ordinales: {len([c for c in ordinal_features if c in df_train.columns])}")
print(f"  • Variables catégorielles nominales: {len(true_categorical)}")

# Analyse de la cardinalité des variables catégorielles
print(f" Cardinalité des variables catégorielles:")
cardinality = pd.DataFrame({
    'Variable': categorical_features,
    'Modalités uniques': [df_train[c].nunique() for c in categorical_features],
    'Type': ['Ordinal' if c in ordinal_features else 'Nominal' for c in categorical_features]
}).sort_values('Modalités uniques', ascending=False)

display(cardinality.head(15))

# Visualisation de la cardinalité
fig, axes = plt.subplots(1, 2, figsize=(18, 8))

# Cardinalité des variables catégorielles
ax1 = axes[0]
high_card = cardinality.head(15)
colors = ['coral' if t == 'Nominal' else 'skyblue' for t in high_card['Type']]
bars = ax1.barh(high_card['Variable'], high_card['Modalités uniques'], color=colors)
ax1.set_xlabel('Nombre de modalités uniques')
ax1.set_title('Cardinalité des variables catégorielles (Top 15)', fontsize=14, fontweight='bold')
ax1.invert_yaxis()

# Légende
legend_elements = [Patch(facecolor='coral', label='Nominal'),
                   Patch(facecolor='skyblue', label='Ordinal')]
ax1.legend(handles=legend_elements, loc='lower right')

# Distribution des types de variables
ax2 = axes[1]
type_counts = [len(numeric_features),
               len([c for c in ordinal_features if c in df_train.columns]),
               len(true_categorical)]
labels = ['Numériques\ncontinues', 'Catégorielles\nordinales', 'Catégorielles\nnominales']
colors_pie = ['lightgreen', 'skyblue', 'coral']
wedges, texts, autotexts = ax2.pie(type_counts, labels=labels, autopct='%1.1f%%',
                                    colors=colors_pie, startangle=90)
ax2.set_title('Distribution des types de variables', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

"""###**1.2. Compréhension de la variable cible**

*Interprétation de la partie*
"""

# Statistiques sur la variable SalePrice
df_train['SalePrice'].describe()

# Histogramme
plt.figure(figsize=(10, 6))
sns.histplot(df_train['SalePrice'], kde=True)
plt.title('Distribution de SalePrice')
plt.xlabel('SalePrice')
plt.ylabel('Fréquence')
plt.show()

# Boxplot
plt.figure(figsize=(10, 6))
sns.boxplot(x=df_train['SalePrice'])
plt.title('Boxplot de SalePrice')
plt.xlabel('SalePrice')
plt.show()

print(f"Skewness of SalePrice: {df_train['SalePrice'].skew():.2f}")

"""Le coefficient d'asymetrie est de **1,88** (positif et élevé), ce qui suppose que la queue de la distribution s'étire vers la droite. La majorité des données sont concentrées à gauche, mais quelques valeurs très élevées tirent la moyenne vers le haut."""

print(f"Kurtosis of SalePrice: {df_train['SalePrice'].kurtosis():.2f}")

"""Le coefficient d'applatissement obtenu (**6,54**) est positif et fort, ce qui suppose une distribution très pointue autour de la moyenne, avec des queues de distribution épaisses, pouvant signifier une forte concentration de valeurs aberrantes (outliers)"""

# Au vu des resultats de l'asymetrie et de l'applatissement, appliquer une transformation logarithmique pourrait aider
# à ameliorer les performances des modèles par la suite.

log_SalePrice = np.log1p(df_train['SalePrice']) # log(1 + x)
plt.figure(figsize=(10, 6))
sns.histplot(log_SalePrice, kde=True)
plt.title('Distribution de log(SalePrice)')
plt.xlabel('log(SalePrice)')
plt.ylabel('Fréquence')

# Boxplot
plt.figure(figsize=(10, 6))
sns.boxplot(x=log_SalePrice)
plt.title('Boxplot de log(SalePrice)')
plt.xlabel('log(SalePrice)')
plt.show()

print(f"Skewness of log(SalePrice): {log_SalePrice.skew():.2f}")

print(f"Kurtosis of log(SalePrice): {log_SalePrice.kurtosis():.2f}")

"""###**1.3. Analyse des variables numériques**

*Interprétation de la partie*

####**Statistiques simples**
"""

# Statistiques des variables
df_stats = df_numericals.describe()

# Ajout du coefficient de variation
vector_cv = df_stats.loc['std'] / df_stats.loc['mean']
df_stats.loc['cv'] = vector_cv

# Ajout des stats sur l'asymetrie et l'applatissement
vector_skew = df_numericals.skew()
vector_kurtosis = df_numericals.kurtosis()
df_stats.loc['skew'] = vector_skew
df_stats.loc['kurtosis'] = vector_kurtosis

# Pourcentage de valeurs nulles
vector_null = df_numericals.isnull().sum() * 100 / len(df_numericals)
df_stats.loc['null_percentage'] = vector_null

df_stats

"""####**Correlation avec `SalePrice`**"""

# Test de correlation de pearson entre les variables num et SalePrice
corr_series = df_numericals.corrwith(df_train['SalePrice'])

# Convertir la série de corrélations en DataFrame pour ajouter une colonne de valeurs absolues
corr_df = corr_series.to_frame(name='correlation')

# Ajouter la colonne des valeurs absolues des correlations
corr_df['abs_correlation'] = corr_df['correlation'].abs()

# Trier le DataFrame par les valeurs absolues des correlations
corr_df.sort_values(by='abs_correlation', ascending=False, inplace=True)
corr_df

# Matrice de corrélation avec SalePrice
print(" Corrélation avec SalePrice (Top 15):")
correlation = df_train[numeric_features + ['SalePrice']].corr()['SalePrice'].sort_values(ascending=False)
correlation_df = pd.DataFrame({
    'Variable': correlation.index[:-1],
    'Corrélation': correlation.values[:-1]
}).head(15)

display(correlation_df.style.background_gradient(cmap='RdYlGn', subset=['Corrélation']))

# Visualisation des corrélations
fig, axes = plt.subplots(2, 2, figsize=(18, 16))

# Heatmap des corrélations (top variables)
ax1 = axes[0, 0]
top_corr_vars = correlation.head(12).index.tolist()
corr_matrix = df_train[top_corr_vars].corr()
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',
            center=0, ax=ax1, square=True)
ax1.set_title('Matrice de corrélation (Top 12 variables)', fontsize=12, fontweight='bold')

# Barplot des corrélations
ax2 = axes[0, 1]
colors = ['darkgreen' if x > 0.5 else 'orange' if x > 0.3 else 'gray'
          for x in correlation_df['Corrélation']]
bars = ax2.barh(correlation_df['Variable'], correlation_df['Corrélation'], color=colors)
ax2.set_xlabel('Coefficient de corrélation')
ax2.set_title('Corrélation avec SalePrice', fontsize=12, fontweight='bold')
ax2.set_xlim(0, 1)
ax2.invert_yaxis()

# Scatter plot: GrLivArea vs SalePrice
ax3 = axes[1, 0]
ax3.scatter(df_train['GrLivArea'], df_train['SalePrice'], alpha=0.5, c='steelblue')
ax3.set_xlabel('Surface habitable au-dessus du sol (sq ft)')
ax3.set_ylabel('Prix de vente ($)')
ax3.set_title('GrLivArea vs SalePrice', fontsize=12, fontweight='bold')

# Identifier les outliers
outliers = df_train[(df_train['GrLivArea'] > 4000) & (df_train['SalePrice'] < 200000)]
ax3.scatter(outliers['GrLivArea'], outliers['SalePrice'],
           c='red', s=100, marker='x', label='Outliers potentiels')
ax3.legend()

# Analyse de la multicolinéarité
print(f"  Paires de variables fortement corrélées (>0.8):")
high_corr_pairs = []
for i in range(len(corr_matrix.columns)):
    for j in range(i+1, len(corr_matrix.columns)):
        if abs(corr_matrix.iloc[i, j]) > 0.8:
            high_corr_pairs.append({
                'Variable 1': corr_matrix.columns[i],
                'Variable 2': corr_matrix.columns[j],
                'Corrélation': corr_matrix.iloc[i, j]
            })

if high_corr_pairs:
    high_corr_df = pd.DataFrame(high_corr_pairs)
    display(high_corr_df.style.background_gradient(cmap='Reds', subset=['Corrélation']))
else:
    print("  Aucune paire avec corrélation > 0.8 dans le top 12")

"""###**1.4. Analyse des variables catégorielles ordinales**

*Interprétation de la partie*
"""

# Boxplot de SalePrice en fonction de chaque variable
df_ordinals['OverallCond'] = df_ordinals['OverallCond'].astype('object')
df_ordinals['OverallQual'] = df_ordinals['OverallQual'].astype('object')

for col in df_ordinals.columns:
    plt.figure(figsize=(10, 6))
    sns.boxplot(y=df_train['SalePrice'], x=df_train[col])
    plt.title(f'Boxplot de SalePrice en fonction de {col}')
    plt.xlabel(col)
    plt.ylabel('SalePrice')
    plt.show()

"""###**1.5. Analyse des variables catégorielles nominales**

*Interprétation de la partie*
"""

df_nominals['SalePrice'] = df_train['SalePrice'] # ajout pour des besoins de calcul

for var in nominal_features:
    print(f"\n{'=**' * 20} Analyse et visualisation de la variable {var} {'**=' * 20}")

    # 1. Nombre de modalités uniques
    num_unique_modalities = df_nominals[var].nunique(dropna=False)
    print(f"Nombre de modalités uniques de {var}: {num_unique_modalities}")

    # 2. Fréquence pour chaque modalité
    value_counts = df_nominals[var].value_counts(normalize=True, dropna=False)
    print("Fréquence de chaque modalité:")
    print(value_counts)

    # 3. Prix moyen associé à chaque modalité
    mean_prices = df_nominals.groupby(var, dropna=False)['SalePrice'].mean()
    print("Prix moyen associé à chaque modalité:")
    print(mean_prices)

    # 4. Générer le bar chart
    fig, axes = plt.subplots(1, 2, figsize=(18, 6))
    fig.suptitle(f'Analyse de {var} par rapport à SalePrice', fontsize=16)

    # Bar plot pour le prix moyen de SalePrice
    sns.barplot(x=mean_prices.index, y=mean_prices.values, palette='viridis', ax=axes[0])
    axes[0].set_title(f'Prix moyen de SalePrice par {var}')
    axes[0].set_xlabel(var)
    axes[0].set_ylabel('Prix moyen de SalePrice')
    axes[0].tick_params(axis='x', rotation=45)
    axes[0].grid(axis='y', linestyle='--', alpha=0.7)

    # Bar plot pour les fréquences
    sns.barplot(x=value_counts.index, y=value_counts.values, palette='magma', ax=axes[1])
    axes[1].set_title(f'Fréquence des modalités de {var}')
    axes[1].set_xlabel(var)
    axes[1].set_ylabel('Fréquence relative')
    axes[1].tick_params(axis='x', rotation=45)
    axes[1].grid(axis='y', linestyle='--', alpha=0.7)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Ajuste le layout pour éviter le chevauchement du titre
    plt.show()

    print(f"{'=**' * 20} Fin de l'analyse pour {var} {'**=' * 20}")

df_nominals.drop(columns=['SalePrice'], inplace=True)

"""###**1.6. Analyse temporelle**

*Interprétation de la partie*
"""

# Copie de df_train
train = df_train.copy()

# Création de variables temporelles
train['HouseAge'] = train['YrSold'] - train['YearBuilt'] # age au moment de la vente
train['RemodAge'] = train['YrSold'] - train['YearRemodAdd'] # années écoulées depuis la derniere renovation au moment de la vente
train['IsNew'] = (train['YrSold'] == train['YearBuilt']).astype(int) # maison nouvelle au moment de la vente

print(f" Statistiques temporelles:")
print(f"  • Âge moyen des maisons: {train['HouseAge'].mean():.1f} ans")
print(f"  • Âge médian: {train['HouseAge'].median():.1f} ans")
print(f"  • Maisons neuves (vendues l'année de construction): {train['IsNew'].sum()} ({train['IsNew'].mean()*100:.1f}%)")

# Visualisations temporelles
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Prix par année de vente
ax1 = axes[0, 0]
yearly_stats = train.groupby('YrSold')['SalePrice'].agg(['mean', 'median', 'count'])
ax1.plot(yearly_stats.index, yearly_stats['mean'], marker='o', label='Moyenne', linewidth=2)
ax1.plot(yearly_stats.index, yearly_stats['median'], marker='s', label='Médiane', linewidth=2)
ax1.set_xlabel('Année de vente')
ax1.set_ylabel('Prix de vente ($)')
ax1.set_title('Évolution des prix par année de vente', fontsize=12, fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Distribution de l'âge des maisons
ax2 = axes[0, 1]
sns.histplot(train['HouseAge'], kde=True, ax=ax2, color='coral', bins=30)
ax2.axvline(train['HouseAge'].mean(), color='red', linestyle='--',
           label=f'Moyenne: {train["HouseAge"].mean():.1f} ans')
ax2.axvline(train['HouseAge'].median(), color='green', linestyle='--',
           label=f'Médiane: {train["HouseAge"].median():.1f} ans')
ax2.set_xlabel('Âge de la maison (ans)')
ax2.set_ylabel('Nombre de maisons')
ax2.set_title('Distribution de l\'âge des maisons', fontsize=12, fontweight='bold')
ax2.legend()

# Prix vs Âge
ax3 = axes[1, 0]
ax3.scatter(train['HouseAge'], train['SalePrice'], alpha=0.4, c='steelblue')
z = np.polyfit(train['HouseAge'], train['SalePrice'], 1)
p = np.poly1d(z)
ax3.plot(train['HouseAge'].sort_values(), p(train['HouseAge'].sort_values()),
         "r--", alpha=0.8, linewidth=2, label=f'Tendance linéaire')
ax3.set_xlabel('Âge de la maison (ans)')
ax3.set_ylabel('Prix de vente ($)')
ax3.set_title('Prix vs Âge de la maison', fontsize=12, fontweight='bold')
ax3.legend()

# Saisonnalité (mois de vente)
ax4 = axes[1, 1]
monthly_sales = train.groupby('MoSold')['SalePrice'].median()
month_names = ['Jan', 'Fév', 'Mar', 'Avr', 'Mai', 'Juin',
               'Juil', 'Août', 'Sep', 'Oct', 'Nov', 'Déc']
ax4.bar(range(1, 13), monthly_sales.values, color=plt.cm.viridis(np.linspace(0, 1, 12)))
ax4.set_xticks(range(1, 13))
ax4.set_xticklabels(month_names)
ax4.set_xlabel('Mois de vente')
ax4.set_ylabel('Prix médian ($)')
ax4.set_title('Saisonnalité des prix de vente', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.savefig('07_temporal_analysis.png', dpi=150, bbox_inches='tight')
plt.show()

print(f" Volume de ventes par année:")
print(yearly_stats[['count', 'mean', 'median']].round(0))

"""###**1.7. Etude de la multicolinéarité**

*Interprétation de la partie*

####**1.7.1. Entre variables numériques**
"""

X_numeric = df_numericals.copy()

# Remplacer les valeurs inf par NA, et supprimer
X_numeric.replace([np.inf, -np.inf], np.nan, inplace=True)
X_numeric.dropna(inplace=True)

# Suppimer les colonnes sans variance
constant_columns = [col for col in X_numeric.columns if X_numeric[col].nunique() == 1]
X_numeric.drop(columns=constant_columns, inplace=True)

# Ajout d'une constante dans le dataframe
X_numeric_const = add_constant(X_numeric)

# Calcul du VIF
vif_data = pd.DataFrame()
vif_data["feature"] = X_numeric_const.columns
vif_data["VIF"] = [variance_inflation_factor(X_numeric_const.values, i)
                   for i in range(X_numeric_const.shape[1])]

# Affichage des top 10
vif_data.sort_values(by="VIF", ascending=False, inplace=True)
print("Top 10 variables with highest VIF scores:")
display(vif_data[vif_data['feature'] != 'const'].head(10))

"""Le calcul précédent du VIF a abouti à des valeurs infinies, indiquant une multicolinéarité parfaite. Ceci est probablement dû au fait que certaines variables sont des sommes directes ou des combinaisons linéaires d'autres variables (par exemple, `TotalBsmtSF`, `GrLivArea`). Pour y remédier, nous supprimerons ces variables linéairement dépendantes (en particulier `TotalBsmtSF `, `GrLivArea`, `1stFlrSF`, `2ndFlrSF`, `BsmtFinSF1`, `BsmtFinSF2`, `BsmtUnfSF`, `LowQualFinSF`) du DataFrame numérique avant de recalculer les VIF."""

X_numeric_cleaned = df_numericals.copy()

# Suprimer les variables causant des VIF inf
X_numeric_cleaned.drop(columns=[
    'TotalBsmtSF', 'GrLivArea', '1stFlrSF', '2ndFlrSF',
    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'LowQualFinSF'
], errors='ignore', inplace=True)

X_numeric_cleaned.replace([np.inf, -np.inf], np.nan, inplace=True)
X_numeric_cleaned.dropna(inplace=True)

constant_columns_cleaned = [col for col in X_numeric_cleaned.columns if X_numeric_cleaned[col].nunique() == 1]
X_numeric_cleaned.drop(columns=constant_columns_cleaned, inplace=True)

X_numeric_const_cleaned = add_constant(X_numeric_cleaned)

# Calcul du VIF
vif_data_cleaned = pd.DataFrame()
vif_data_cleaned["feature"] = X_numeric_const_cleaned.columns
vif_data_cleaned["VIF"] = [variance_inflation_factor(X_numeric_const_cleaned.values, i)
                   for i in range(X_numeric_const_cleaned.shape[1])]

vif_data_cleaned.sort_values(by="VIF", ascending=False, inplace=True)
print("Top 10 des variables avec les VIF les plus élevés:")
display(vif_data_cleaned[vif_data_cleaned['feature'] != 'const'].head(10))

# Calcul de la matrice de correlation
correlation_matrix = X_numeric_cleaned.corr()

# Affichage du heatmap
plt.figure(figsize=(20, 18))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Matrice de correlation des variables numériques', fontsize=20)
plt.show()

# Identification des pairs les plus correlées (correlation absolue > 0.8)
highly_correlated_pairs = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i + 1, len(correlation_matrix.columns)):
        if abs(correlation_matrix.iloc[i, j]) > 0.8:
            highly_correlated_pairs.append({
                'var 1': correlation_matrix.columns[i],
                'var 2': correlation_matrix.columns[j],
                'Correlation': correlation_matrix.iloc[i, j]
            })

highly_correlated_df = pd.DataFrame(highly_correlated_pairs)

print("\nPairs les plus correlées (correlation absolue > 0.8):")
display(highly_correlated_df)

"""####**1.7.2. Entre variables numériques et variables catégorielles**"""



"""####**1.7.3. Entre variables catégorielles**"""

# Categories de la variable Neighboorhood
neighboorhood_df = df_train['Neighborhood'].value_counts(normalize=True, dropna=False).reset_index()
# neighboorhood_df['percent'] = neighboorhood_df['count'] / neighboorhood_df['count'].sum() * 100
neighboorhood_df.sort_values(by='proportion', ascending=False, inplace=True)
neighboorhood_df

"""##**2. Prétraitement**

*Description de la partie et résumé des différentes étapes et principales transformations effectuées*

### **2.1. Séparation des données**
"""

# A supprimer
df_train = pd.read_csv('train.csv')
df_train.drop(columns=['Id'], inplace=True)
df_train.head(3)

df = df_train
df.head(3)

from sklearn.model_selection import train_test_split

X = df.drop('SalePrice', axis=1)
y = df['SalePrice']

# Séparation du jeu de données
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)

print(f"Dimension du train: X_train  {X_train.shape} et y_train {y_train.shape}")
print(f"Dimension du test: X_test {X_test.shape} et y_test {y_test.shape}")

"""### **2.2 Traitement des valeurs manquantes**"""

# Classification des variables manquantes selon leur nature
print(f" Classification des valeurs manquantes:")

# NA = Absence d'équipements (None) --- variables catégorielles
none_features = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',
                 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',
                 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',
                 'BsmtFinType2', 'MasVnrType']

# NA = 0 (pas d'équipements) --- variables numériques
zero_features = ['GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1',
                 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',
                 'BsmtHalfBath', 'MasVnrArea']

# NA à imputer par médiane/groupe (lien fort entre LotFrontage et Neighborhood)
# On imputera LotFrontage en fonction de Neighborhood
group_impute = {'LotFrontage': 'Neighborhood'}

# NA à imputer par mode --- variables catégorielles
mode_features = ['MSZoning', 'Functional', 'Utilities', 'SaleType',
                 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'Electrical']

print(f"  • Variables 'None' (absence d'équipement): {len(none_features)}")
print(f"  • Variables '0' (surface/quantité nulle): {len(zero_features)}")
print(f"  • Variables imputation par groupe: {len(group_impute)}")
print(f"  • Variables imputation par mode: {len(mode_features)}")

"""####**Classe pour la traitement des valeurs manquantes**"""

import logging
from sklearn.base import BaseEstimator, TransformerMixin

class MissingValuesHandler(BaseEstimator, TransformerMixin):
  def __init__(self, none_features, zero_features, group_impute, mode_features, strategy_lotfrontage="median", neighborhoods_threshold = 0.02):
    """Cette classe permet de traiter les missing values et elle pourra être inclus dans un Pipeline scikit-learn"""
    self.none_features = none_features
    self.zero_features = zero_features
    self.group_impute = group_impute
    self.mode_features = mode_features
    self.correct_neighborhoods_ = None
    self.neighborhoods_threshold = neighborhoods_threshold
    self.strategy_lotfrontage = strategy_lotfrontage # median / mean
    self.global_stat_lotfrontage_ = None
    self.stat_lotfrontage_per_neighborhood_ = {} # median / mean de lotfrontage par neighbourhood
    self.mode_for_mode_features_ = {}


  def fit(self, X_train, y=None):
    """Calcul des paramètres pour l'imputation à partir du jeu d'entraînement"""
    print("Missing Values Hander starting in fit...")
    logging.info("Calcul des paramètres pour l'imputation à partir du jeu d'entraînement...")
    if 'Neighborhood' in X_train.columns:
      # Modalités assez représentéess (% > self.neighborhoods_threshold)
      neighborhood_proportions = X_train['Neighborhood'].value_counts(normalize=True)
      correct_neighborhoods = neighborhood_proportions[neighborhood_proportions >= self.neighborhoods_threshold].index
      correct_neighborhoods = list(correct_neighborhoods)
      self.correct_neighborhoods_ = correct_neighborhoods

    # Mode for mode_features
    if self.mode_features and len(self.mode_features) > 0:
      result = {}
      for feature in self.mode_features:
        mode_value = X_train[feature].mode()[0]
        result[feature] = mode_value
      self.mode_for_mode_features_ = result

      # stat global pour lotfrontage
      if 'LotFrontage' in X_train.columns:
        if self.strategy_lotfrontage == 'median':
          self.global_stat_lotfrontage_ = X_train['LotFrontage'].median()
        elif self.strategy_lotfrontage == 'mean':
          self.global_stat_lotfrontage_ = X_train['LotFrontage'].mean()
        else:
          logging.error("Mauvaise valeur de strategy_lotfrontage: mettre mean ou median")

      # Stat de lotfrontage par neighborhoods
      if 'Neighborhood' in X_train.columns and 'LotFrontage' in X_train.columns and self.correct_neighborhoods_ is not None:
        X_train_temp = X_train.copy()
        # Conserver uniquement les correct_neighborhoods, les autres sont groupés en 'Autres'
        all_unique_neighborhoods = set(X_train_temp['Neighborhood'].unique())
        categories_to_replace = list(all_unique_neighborhoods.difference(set(self.correct_neighborhoods_)))
        X_train_temp['Neighborhood'] = X_train_temp['Neighborhood'].replace(categories_to_replace, 'Autres')

        effective_neighborhoods = list(set(self.correct_neighborhoods_ + ['Autres']))

        for neighborhood in effective_neighborhoods:
          neighborhood_data = X_train_temp[X_train_temp['Neighborhood'] == neighborhood]
          if self.strategy_lotfrontage == 'median':
            self.stat_lotfrontage_per_neighborhood_[neighborhood] = neighborhood_data['LotFrontage'].median()
          elif self.strategy_lotfrontage == 'mean':
            self.stat_lotfrontage_per_neighborhood_[neighborhood] = neighborhood_data['LotFrontage'].mean()
          else:
            logging.error("Mauvaise valeur de strategy_lotfrontage: mettre mean ou median")

    return self # Important pour la compatibilité avec scikit-learn

  def transform(self, X):
    """Applique diverses transformations pour gérer les valeurs manquantes"""
    X = X.copy() # Travailler sur une copie pour éviter les SettingWithCopyWarning
    print("Missing Values Hander starting in transform...")
    logging.info("Imputation des valeurs manquantes en cours...")
    missing_before = X.isnull().sum().sum()
    logging.info(f"  • Valeurs manquantes avant: {missing_before:,}")

    logging.info("1. Imputation - ÉTAPE 1: NA = 'None' (Absence d'équipements)")
    if self.none_features and len(self.none_features) > 0:
      for feature in self.none_features:
        if feature in X.columns:
            X[feature] = X[feature].fillna('None')
            logging.info(f"  ✓ {feature}: NA → 'None'")

    logging.info("2. Imputation - ÉTAPE 2: NA = 0 (Quantité nulle)")
    if self.zero_features and len(self.zero_features) > 0:
      for feature in self.zero_features:
        if feature in X.columns:
            X[feature] = X[feature].fillna(0)
            logging.info(f"  ✓ {feature}: NA → 0")

    logging.info("3. Imputation - ÉTAPE 3: LotFrontage par groupe (Neighborhood)")
    # remplacer les categories mal représentées par 'Autres'
    # D'abord, identifier les catégories qui ne sont PAS dans `correct_neighborhoods`
    if self.correct_neighborhoods_ is not None and 'Neighborhood' in X.columns:
      all_neighborhoods = set(X['Neighborhood'].unique())
      categories_to_replace = list(all_neighborhoods.difference(set(self.correct_neighborhoods_)))
      X['Neighborhood'] = X['Neighborhood'].replace(categories_to_replace, 'Autres')

      # Imputation par médiane / moyenne de groupe
      # On utilisera le dictionnaire self.stat_lotfrontage_per_neighborhood_
      mapped_lotfrontage = X['Neighborhood'].map(self.stat_lotfrontage_per_neighborhood_)
      X['LotFrontage'] = X['LotFrontage'].fillna(mapped_lotfrontage)

      # Si encore des NA (Neighborhood avec tous les NA), imputer par médiane globale
      remaining_na = X['LotFrontage'].isnull().sum()
      if remaining_na > 0:
          if self.global_stat_lotfrontage_ is not None:
            X['LotFrontage'] = X['LotFrontage'].fillna(self.global_stat_lotfrontage_)
            logging.info(f"   {remaining_na} valeurs imputées par {self.strategy_lotfrontage} globale ({self.global_stat_lotfrontage_:.1f})")
    else:
      logging.info("  !!! LotFrontage: NA → aucune imputation car, pas de Neighborhood ou de correct_neighborhoods spécifié ")

    logging.info("4. IMPUTATION - ÉTAPE 4: Variables catégorielles par mode")
    if self.mode_for_mode_features_:
      for feature in self.mode_features:
        if feature in X.columns:
            mode_value = self.mode_for_mode_features_[feature]
            X[feature] = X[feature].fillna(mode_value)
    else:
      logging.info("  !!! Variables catégorielles par mode: NA → aucune imputation car, pas de mode_for_mode_features spécifié ")

    na_final = X.isnull().sum().sum()
    logging.info(f"  ✓ Valeurs manquantes après:  {na_final:,}")

    return X

  # fit_transform est fourni par TransformerMixin, donc pas besoin de le redéfinir sauf si logique spécifique

"""### **2.3 Correction des anomalies et incohérences**"""

def correct_anomaly(X):
  invalid_garage_year = (X['GarageYrBlt'] > X['YearBuilt']).sum()
  if invalid_garage_year > 0:
      X['GarageYrBlt'] = np.where(
          X['GarageYrBlt'] > X['YearBuilt'],
          X['YearBuilt'],
          X['GarageYrBlt']
      )
      print(f"  ✓ {invalid_garage_year} valeurs corrigées (GarageYrBlt > YearBuilt)")
  return X

class AnomalyCorrector(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X = X.copy()
        print("Anomaly Corrector Handler starting...")

        mask = X['GarageYrBlt'] > X['YearBuilt']
        X.loc[mask, 'GarageYrBlt'] = X.loc[mask, 'YearBuilt']

        # Remplacer
        #X.replace([np.inf, -np.inf], np.nan, inplace=True)

        return X

# print("\n" + "=" * 80)
# print("2.3 CORRECTION DES ANOMALIES ET INCOHÉRENCES")
# print("=" * 80)

# anomalies_detected = []

# # 1. GarageYrBlt > YearBuilt
# print(f" 1. Vérification GarageYrBlt > YearBuilt:")
# invalid_garage_year = (all_data['GarageYrBlt'] > all_data['YearBuilt']).sum()
# if invalid_garage_year > 0:
#     all_data['GarageYrBlt'] = np.where(
#         all_data['GarageYrBlt'] > all_data['YearBuilt'],
#         all_data['YearBuilt'],
#         all_data['GarageYrBlt']
#     )
#     print(f"  ✓ {invalid_garage_year} valeurs corrigées (GarageYrBlt > YearBuilt)")
#     anomalies_detected.append(f"GarageYrBlt > YearBuilt: {invalid_garage_year} cas")

# # 2. MasVnrArea > 0 mais MasVnrType = 'None' //****** // Il n'y en a pas
# print(f" Vérification MasVnrArea > 0 avec MasVnrType = 'None':")
# inconsistent_masvnr = ((all_data['MasVnrArea'] > 0) & (all_data['MasVnrType'] == 'None')).sum()
# if inconsistent_masvnr > 0:
#     all_data.loc[(all_data['MasVnrArea'] > 0) & (all_data['MasVnrType'] == 'None'), 'MasVnrType'] = 'BrkFace'
#     print(f"  ✓ {inconsistent_masvnr} valeurs corrigées (MasVnrType 'None' → 'BrkFace')")
#     anomalies_detected.append(f"MasVnrArea>0 & Type=None: {inconsistent_masvnr} cas")

# # 3. GarageArea > 0 mais GarageCars = 0 (ou inverse) /************/ Il n'y en a pas
# print(f" Vérification incohérences GarageArea/GarageCars:")
# garage_inconsistency = ((all_data['GarageArea'] > 0) & (all_data['GarageCars'] == 0)).sum()
# if garage_inconsistency > 0:
#     # Imputer 1 voiture si surface > 0 mais 0 voiture
#     all_data.loc[(all_data['GarageArea'] > 0) & (all_data['GarageCars'] == 0), 'GarageCars'] = 1
#     print(f"  ✓ {garage_inconsistency} valeurs corrigées (GarageCars 0 → 1)")
#     anomalies_detected.append(f"GarageArea>0 & Cars=0: {garage_inconsistency} cas")

# # 4. Vérification des surfaces négatives (ne devrait pas exister) /** Il n'y en a pas ***/
# print(f" Vérification surfaces négatives:")
# surface_cols = ['GrLivArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GarageArea']
# negative_surfaces = 0
# for col in surface_cols:
#     neg_count = (all_data[col] < 0).sum()
#     if neg_count > 0:
#         negative_surfaces += neg_count
#         print(f"  {col}: {neg_count} valeurs négatives détectées")

# if negative_surfaces == 0:
#     print(f"  ✓ Aucune surface négative détectée")

# print(f"\n{'='*60}")
# print("RÉSUMÉ DES CORRECTIONS:")
# for anomaly in anomalies_detected:
#     print(f"  • {anomaly}")
# if not anomalies_detected:
#     print(f"  • Aucune anomalie majeure détectée")
# print(f"{'='*60}")

"""### **2.4 Création de nouvelles features**"""

class FeatureEngineer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X = X.copy()
        print("Feature Engineering Handler starting...")

        # Surfaces
        X['TotalSF'] = X['TotalBsmtSF'] + X['1stFlrSF'] + X['2ndFlrSF']
        X['TotalSF_AboveGround'] = X['1stFlrSF'] + X['2ndFlrSF']
        X['Has2ndFloor'] = (X['2ndFlrSF'] > 0).astype(int)
        X['HasBasement'] = (X['TotalBsmtSF'] > 0).astype(int)

        # Porches
        X['TotalPorchSF'] = (
            X['OpenPorchSF'] + X['3SsnPorch'] +
            X['EnclosedPorch'] + X['ScreenPorch'] +
            X['WoodDeckSF']
        )
        X['HasPorch'] = (X['TotalPorchSF'] > 0).astype(int)
        X['HasDeck'] = (X['WoodDeckSF'] > 0).astype(int)
        X['HasPool'] = (X['PoolArea'] > 0).astype(int)

        # Temps
        X['HouseAge'] = X['YrSold'] - X['YearBuilt']
        X['RemodAge'] = X['YrSold'] - X['YearRemodAdd']
        X['IsNew'] = (X['YrSold'] == X['YearBuilt']).astype(int)
        X['HasBeenRemod'] = (X['YearRemodAdd'] > X['YearBuilt']).astype(int)
        X['HouseAgeBin'] = pd.cut(
            X['HouseAge'],
            bins=[0, 5, 20, 50, 100, 200],
            labels=['New', 'Recent', 'Moderate', 'Old', 'VeryOld'],
            include_lowest=True
        )


        # Garage
        X['HasGarage'] = (X['GarageArea'] > 0).astype(int)
        X['GarageAge'] = (X['YrSold'] - X['GarageYrBlt']).clip(lower=0)

        # Cheminée
        fireplace_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}
        X['HasFireplace'] = (X['Fireplaces'] > 0).astype(int)
        X['FireplaceScore'] = X['Fireplaces'] * X['FireplaceQu'].map(fireplace_map).fillna(0)

        # Suppressions
        drop_cols = [
            '1stFlrSF', '2ndFlrSF', 'TotalBsmtSF',
            'OpenPorchSF', '3SsnPorch', 'EnclosedPorch',
            'YearBuilt', 'YearRemodAdd', 'YrSold',
            'GarageYrBlt', 'Fireplaces'
        ]
        X = X.drop(columns=[c for c in drop_cols if c in X.columns])

        return X

def features_creation(X):
  features_to_delete = []
  # 1. FEATURES DE SURFACE TOTALE
  print(f"\n   1. Features de surface:")
  X['TotalSF'] = X['TotalBsmtSF'] + X['1stFlrSF'] + X['2ndFlrSF']
  X['TotalSF_AboveGround'] = X['1stFlrSF'] + X['2ndFlrSF']
  X['Has2ndFloor'] = (X['2ndFlrSF'] > 0).astype(int)
  X['HasBasement'] = (X['TotalBsmtSF'] > 0).astype(int)
  features_to_delete.extend(['1stFlrSF', '2ndFlrSF', 'TotalBsmtSF'])

  print(f"     ✓ TotalSF (surface totale)")
  print(f"     ✓ TotalSF_AboveGround (surface hors-sol)")
  print(f"     ✓ Has2ndFloor (binaire)")
  print(f"     ✓ HasBasement (binaire)")

  # 2. FEATURES DE SALLES DE BAIN
  # print(f" Features de salles de bain:")
  # X['TotalBath'] = (X['FullBath'] + 0.5 * X['HalfBath'] +
  #                         X['BsmtFullBath'] + 0.5 * X['BsmtHalfBath'])
  # X['TotalFullBath'] = X['FullBath'] + X['BsmtFullBath']
  # X['TotalHalfBath'] = X['HalfBath'] + X['BsmtHalfBath']

  # print(f"     ✓ TotalBath (équivalent full bath)")
  # print(f"     ✓ TotalFullBath, TotalHalfBath")

  # 3. FEATURES DE PORCHES ET EXTÉRIEUR
  print(f" Features d'extérieur:")
  X['TotalPorchSF'] = (X['OpenPorchSF'] + X['3SsnPorch'] +
                              X['EnclosedPorch'] + X['ScreenPorch'] +
                              X['WoodDeckSF'])
  X['HasPorch'] = (X['TotalPorchSF'] > 0).astype(int)
  X['HasDeck'] = (X['WoodDeckSF'] > 0).astype(int)
  X['HasPool'] = (X['PoolArea'] > 0).astype(int)
  features_to_delete.extend(['OpenPorchSF', '3SsnPorch', 'EnclosedPorch'])

  print(f"     ✓ TotalPorchSF (surface porches totale)")
  print(f"     ✓ HasPorch, HasDeck, HasPool (binaires)")

  # 4. FEATURES TEMPORELLES
  print(f" Features temporelles:")
  X['HouseAge'] = X['YrSold'] - X['YearBuilt']
  X['RemodAge'] = X['YrSold'] - X['YearRemodAdd']
  X['IsNew'] = (X['YrSold'] == X['YearBuilt']).astype(int)
  X['HasBeenRemod'] = (X['YearRemodAdd'] > X['YearBuilt']).astype(int)
  X['HouseAgeBin'] = pd.cut(
            X['HouseAge'],
            bins=[0, 5, 20, 50, 100, 200],
            labels=['New', 'Recent', 'Moderate', 'Old', 'VeryOld'],
            include_lowest=True
        )
  features_to_delete.extend(['YearBuilt', 'YearRemodAdd', 'YrSold'])

  print(f"     ✓ HouseAge (âge de la maison)")
  print(f"     ✓ RemodAge (âge depuis rénovation)")
  print(f"     ✓ IsNew, HasBeenRemod (binaires)")
  print(f"     ✓ HouseAgeBin (catégoriel)")

  # 5. FEATURES DE QUALITÉ PONDÉRÉE
  # print(f" Features de qualité:")
  # X['QualCond'] = X['OverallQual'] * X['OverallCond']
  # X['QualSF'] = X['OverallQual'] * X['TotalSF']
  # X['QualLivArea'] = X['OverallQual'] * X['GrLivArea']

  # print(f"     ✓ QualCond (Qualité × Condition)")
  # print(f"     ✓ QualSF (Qualité × Surface totale)")
  # print(f"     ✓ QualLivArea (Qualité × Surface habitable)")

  # 6. FEATURES DE GARAGE
  print(f" Features de garage:")
  X['HasGarage'] = (X['GarageArea'] > 0).astype(int)
  X['GarageAge'] = X['YrSold'] - X['GarageYrBlt']
  X['GarageAge'] = X['GarageAge'].clip(lower=0)  # Pas d'âge négatif
  features_to_delete.extend(['GarageYrBlt'])

  print(f"     ✓ HasGarage (binaire)")
  print(f"     ✓ GarageAge (âge du garage)")

  # 7. FEATURES DE LOT
  # print(f" Features de terrain:")
  # X['LotFrontageRatio'] = X['LotFrontage'] / np.sqrt(X['LotArea'])
  # X['LotShapeReg'] = (X['LotShape'] == 'Reg').astype(int)

  # print(f"     ✓ LotFrontageRatio (ratio frontage/surface)")
  # print(f"     ✓ LotShapeReg (binaire)")

  # 8. FEA Features de cheminée:")
  X['HasFireplace'] = (X['Fireplaces'] > 0).astype(int)
  X['FireplaceScore'] = X['Fireplaces'] * X['FireplaceQu'].map(
      {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}
  ).fillna(0)
  features_to_delete.extend(['Fireplaces'])

  print(f"     ✓ HasFireplace (binaire)")
  print(f"     ✓ FireplaceScore (nombre × qualité)")

  print(f"\n{'='*60}")
  print(f"RÉSUMÉ DU FEATURE ENGINEERING:")
  print(f"  • Variables initiales: {X.shape[1] - 25}")
  print(f"  • Nouvelles features créées: 25")
  print(f"  • Total features après: {X.shape[1]}")
  print(f"{'='*60}")

  # Afficher les nouvelles features
  new_features = ['TotalSF', 'TotalSF_AboveGround', 'Has2ndFloor', 'HasBasement',
                  'TotalBath', 'TotalFullBath', 'TotalHalfBath', 'TotalPorchSF',
                  'HasPorch', 'HasDeck', 'HasPool', 'HouseAge', 'RemodAge',
                  'IsNew', 'HasBeenRemod', 'HouseAgeBin', 'QualCond', 'QualSF',
                  'QualLivArea', 'HasGarage', 'GarageAge', 'LotFrontageRatio',
                  'LotShapeReg', 'HasFireplace', 'FireplaceScore']

  print(f" Liste des nouvelles features:")
  for i, feat in enumerate(new_features, 1):
      print(f"  {i:2d}. {feat}")

  # Supprimer les features substituées
  X = X.drop(features_to_delete, axis=1)

  return X

"""### **2.5 Transformations des variables ordinales**"""

class OrdinalEncoderCustom(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X = X.copy()
        print("Ordinal Encoder Handler starting...")

        quality_mapping = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}
        exposure_mapping = {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}

        garage_finish_mapping = {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}
        functional_mapping = {'Sal': 0, 'Sev': 1, 'Maj2': 2, 'Maj1': 3,
                              'Mod': 4, 'Min2': 5, 'Min1': 6, 'Typ': 7}
        slope_mapping = {'Sev': 0, 'Mod': 1, 'Gtl': 2}
        shape_mapping = {'IR3': 0, 'IR2': 1, 'IR1': 2, 'Reg': 3}
        contour_mapping = {'Low': 0, 'HLS': 1, 'Bnk': 2, 'Lvl': 3}
        house_age_mapping = {'New': 0, 'Recent': 1, 'Moderate': 2, 'Old': 3, 'VeryOld': 4}
        fence_mapping = {'None': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4}

        bsmt_fin_type_mapping = {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}

        for col in ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',
                    'HeatingQC', 'KitchenQual', 'FireplaceQu',
                    'GarageQual', 'GarageCond', 'PoolQC']:
            if col in X.columns:
                X[col] = X[col].map(quality_mapping)

        if 'BsmtExposure' in X.columns:
            X['BsmtExposure'] = X['BsmtExposure'].map(exposure_mapping)
        if 'GarageFinish' in X.columns:
            X['GarageFinish'] = X['GarageFinish'].map(garage_finish_mapping)
        if 'Functional' in X.columns:
            X['Functional'] = X['Functional'].map(functional_mapping)
        if 'LandSlope' in X.columns:
            X['LandSlope'] = X['LandSlope'].map(slope_mapping)
        if 'LotShape' in X.columns:
            X['LotShape'] = X['LotShape'].map(shape_mapping)
        if 'LandContour' in X.columns:
            X['LandContour'] = X['LandContour'].map(contour_mapping)
        if 'HouseAgeBin' in X.columns:
            X['HouseAgeBin'] = X['HouseAgeBin'].map(house_age_mapping)
        if 'Fence' in X.columns:
            X['Fence'] = X['Fence'].map(fence_mapping)

        # Apply specific basement finish type mapping
        for col in ['BsmtFinType1', 'BsmtFinType2']:
            if col in X.columns:
                X[col] = X[col].map(bsmt_fin_type_mapping)
                print(f"  ✓ {col}: mapping finition sous-sol appliqué")

        # A supprimer
        obj_cols = X.select_dtypes(include='object').columns
        print("Colonnes encore en object :", obj_cols)

        # A supprimer
        for col in X.columns:
          if X[col].dtype == 'object':
              if (X[col] == 'None').any():
                  print(f"⚠️ 'None' encore présent dans {col}")

        return X

# Qualité générale
qual_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',
             'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual',
             'GarageCond', 'PoolQC']

def encode_ordinals(X, qual_cols = qual_cols):

  # Définition des mappings
  quality_mapping = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}
  exposure_mapping = {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}
  finish_mapping = {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}
  functional_mapping = {'Sal': 0, 'Sev': 1, 'Maj2': 2, 'Maj1': 3,
                        'Mod': 4, 'Min2': 5, 'Min1': 6, 'Typ': 7}
  slope_mapping = {'Sev': 0, 'Mod': 1, 'Gtl': 2}
  shape_mapping = {'IR3': 0, 'IR2': 1, 'IR1': 2, 'Reg': 3}
  contour_mapping = {'Low': 0, 'HLS': 1, 'Bnk': 2, 'Lvl': 3}

  print(f" Application des mappings ordinaux:")

  for col in qual_cols:
      if col in X.columns:
          X[col] = X[col].map(quality_mapping)
          print(f"  ✓ {col}: mapping qualité appliqué")

  # Exposition sous-sol
  if 'BsmtExposure' in X.columns:
      X['BsmtExposure'] = X['BsmtExposure'].map(exposure_mapping)
      print(f"  ✓ BsmtExposure: mapping exposition appliqué")

  # Finition garage
  if 'GarageFinish' in X.columns:
      X['GarageFinish'] = X['GarageFinish'].map(finish_mapping)
      print(f"  ✓ GarageFinish: mapping finition appliqué")

  # Fonctionnalité
  if 'Functional' in X.columns:
      X['Functional'] = X['Functional'].map(functional_mapping)
      print(f"  ✓ Functional: mapping fonctionnalité appliqué")

  # Pente du terrain
  if 'LandSlope' in X.columns:
      X['LandSlope'] = X['LandSlope'].map(slope_mapping)
      print(f"  ✓ LandSlope: mapping pente appliqué")

  # Forme du terrain
  if 'LotShape' in X.columns:
      X['LotShape'] = X['LotShape'].map(shape_mapping)
      print(f"  ✓ LotShape: mapping forme appliqué")

  # Contour du terrain
  if 'LandContour' in X.columns:
      X['LandContour'] = X['LandContour'].map(contour_mapping)
      print(f"  ✓ LandContour: mapping contour appliqué")

  # Vérification des conversions
  print(f"\n{'='*60}")
  print("VÉRIFICATION DES CONVERSIONS:")
  ordinal_converted = qual_cols + ['BsmtExposure', 'GarageFinish', 'Functional',
                                    'LandSlope', 'LotShape', 'LandContour']
  for col in ordinal_converted:
      if col in X.columns:
          unique_vals = sorted(X[col].dropna().unique())
          print(f"  • {col}: {unique_vals}")
  print(f"{'='*60}")

  return X

"""### **2.6 Encodage des variables catégorielles nominales** (plus utilisé actuellement)"""

print("\n" + "=" * 80)
print("2.6 ENCODAGE DES VARIABLES CATÉGORIELLES NOMINALES")
print("=" * 80)

# Identifier les variables catégorielles restantes
categorical_features = all_data.select_dtypes(include=['object']).columns.tolist()
print(f" Variables catégorielles à encoder: {len(categorical_features)}")
print(f"   {categorical_features}")

# Analyse de la cardinalité
print(f" Cardinalité des variables catégorielles:")
cardinality = pd.DataFrame({
    'Variable': categorical_features,
    'Modalités': [all_data[c].nunique() for c in categorical_features],
    'Exemples': [str(all_data[c].unique()[:3].tolist()) for c in categorical_features]
}).sort_values('Modalités', ascending=False)

display(cardinality)

# Séparation selon la cardinalité
high_cardinality = cardinality[cardinality['Modalités'] > 10]['Variable'].tolist()
low_cardinality = cardinality[cardinality['Modalités'] <= 10]['Variable'].tolist()

print(f"\n  • Haute cardinalité (> 10): {len(high_cardinality)} variables")
print(f"    {high_cardinality}")
print(f"\n  • Basse cardinalité (≤ 10): {len(low_cardinality)} variables")
print(f"    {low_cardinality}")

# One-Hot Encoding pour basse cardinalité
print(f" One-Hot Encoding pour variables à basse cardinalité...")
all_data = pd.get_dummies(all_data, columns=low_cardinality, drop_first=True)
print(f"   {len(low_cardinality)} variables encodées")
print(f"  Dimensions après OHE: {all_data.shape}")

# Target Encoding / Frequency Encoding pour haute cardinalité
print(f" Encoding pour haute cardinalité (Neighborhood, Exterior...)")

# Pour Neighborhood: utiliser la moyenne de SalePrice du train
if 'Neighborhood' in all_data.columns:
    # Calculer sur le train uniquement
    neighborhood_target_mean = y_train.groupby(all_data.iloc[:ntrain]['Neighborhood']).mean()
    all_data['Neighborhood_TargetEnc'] = all_data['Neighborhood'].map(neighborhood_target_mean)

    # Frequency encoding comme backup
    neighborhood_freq = all_data['Neighborhood'].value_counts(normalize=True)
    all_data['Neighborhood_FreqEnc'] = all_data['Neighborhood'].map(neighborhood_freq)

    all_data = all_data.drop('Neighborhood', axis=1)
    print(f"  ✓ Neighborhood: Target Encoding + Frequency Encoding")

# Pour Exterior1st/2nd: regrouper les rares
for col in ['Exterior1st', 'Exterior2nd']:
    if col in all_data.columns:
        # Garder les 10 plus fréquents, regrouper le reste
        top_10 = all_data[col].value_counts().head(10).index
        all_data[col] = all_data[col].apply(lambda x: x if x in top_10 else 'Other')

        # One-hot après regroupement
        all_data = pd.get_dummies(all_data, columns=[col], prefix=[col])
        print(f"  ✓ {col}: regroupement des rares + OHE")

print(f"\n{'='*60}")
print(f"DIMENSIONS FINALES APRÈS ENCODAGE:")
print(f"  • Variables: {all_data.shape[1]}")
print(f"  • Observations: {all_data.shape[0]}")
print(f"{'='*60}")

"""### **2.7 Transformation des variables numériques (skewness)** (plus utilisé actuellement)"""

print("\n" + "=" * 80)
print("2.7 TRANSFORMATION DES VARIABLES NUMÉRIQUES (SKEWNESS)")
print("=" * 80)

# Identifier les variables numériques fortement asymétriques
numeric_features = all_data.select_dtypes(include=[np.number]).columns.tolist()

skewness_before = []
for col in numeric_features:
    if all_data[col].min() >= 0 and all_data[col].nunique() > 10:  # Éviter les binaires
        skew_val = skew(all_data[col].dropna())
        if abs(skew_val) > 0.75:
            skewness_before.append({'Variable': col, 'Skewness': skew_val})

skew_df = pd.DataFrame(skewness_before).sort_values('Skewness', key=abs, ascending=False)
print(f" Variables fortement asymétriques (|skew| > 0.75): {len(skew_df)}")
display(skew_df.head(15))

# Visualisation avant transformation
fig, axes = plt.subplots(3, 4, figsize=(20, 15))
axes = axes.flatten()

top_skewed = skew_df.head(6)['Variable'].tolist()

for idx, var in enumerate(top_skewed):
    if idx >= 6:
        break

    # Avant transformation
    ax_before = axes[idx * 2]
    sns.histplot(all_data[var], kde=True, ax=ax_before, color='coral', bins=30)
    ax_before.set_title(f'{var}\nSkew: {skew(all_data[var]):.2f}',
                       fontsize=10, fontweight='bold')

    # Après transformation log
    ax_after = axes[idx * 2 + 1]
    # Ajouter 1 pour éviter log(0)
    log_var = np.log1p(all_data[var])
    sns.histplot(log_var, kde=True, ax=ax_after, color='darkgreen', bins=30)
    ax_after.set_title(f'log({var} + 1)\nSkew: {skew(log_var):.2f}',
                      fontsize=10, fontweight='bold')

plt.tight_layout()
plt.show()

# Application de la transformation log
print(f"\n🔧 Application de la transformation log1p...")
skewed_features = skew_df['Variable'].tolist()

for feature in skewed_features:
    # Vérifier qu'il n'y a pas de valeurs négatives
    if all_data[feature].min() >= 0:
        all_data[feature] = np.log1p(all_data[feature])
        print(f"  ✓ {feature}: log1p appliqué")

# Vérification après transformation
print(f"\n{'='*60}")
print("VÉRIFICATION APRÈS TRANSFORMATION:")
skewness_after = []
for var in top_skewed[:5]:
    new_skew = skew(all_data[var].dropna())
    print(f"  • {var}: {skew_df[skew_df['Variable']==var]['Skewness'].values[0]:.2f} → {new_skew:.2f}")
print(f"{'='*60}")

"""### **2.8 Standardisation** (plus utilisé)"""

def custom_standardisation(X, X_train):
  # Séparer les features à scaler (exclure les binaires et déjà normalisées)
  features_to_scale = []
  binary_features = []

  for col in X.columns:
      if pd.api.types.is_numeric_dtype(X[col]): # Ensure the column is numeric
          unique_vals = X[col].nunique()
          if unique_vals == 2 and set(X[col].dropna().unique()).issubset({0, 1}):
              binary_features.append(col)
          elif col not in ['MSSubClass']:
              features_to_scale.append(col)

  print(f" Classification des features:")
  print(f"  • Features à scaler (RobustScaler): {len(features_to_scale)}")
  print(f"  • Features binaires (pas de scaling): {len(binary_features)}")

  # Application du RobustScaler (moins sensible aux outliers)
  print(f" Application du RobustScaler...")
  scaler = RobustScaler()

  # Fit sur train, transform sur tout
  X_scaled = X.copy()
  X_scaled[features_to_scale] = scaler.fit_transform(X[features_to_scale])

  print(f"  ✓ Scaling terminé")

  # Vérification
  print(f"\n{'='*60}")
  print("STATISTIQUES APRÈS SCALING (échantillon):")
  sample_cols = features_to_scale[:5]
  stats_after = X_scaled[sample_cols].describe().round(3)
  print(stats_after)
  print(f"{'='*60}")

  # # Visualisation de la distribution avant/après scaling
  # fig, axes = plt.subplots(2, 3, figsize=(18, 10))

  # sample_features = ['GrLivArea', 'TotalSF', 'LotArea']

  # for idx, feat in enumerate(sample_features):
  #     if feat in all_data.columns:
  #         # Avant scaling
  #         ax1 = axes[0, idx]
  #         sns.histplot(all_data[feat], kde=True, ax=ax1, color='steelblue', bins=30)
  #         ax1.set_title(f'{feat} - Avant scaling\nMean: {all_data[feat].mean():.1f}, Std: {all_data[feat].std():.1f}',
  #                     fontsize=10)

  #         # Après scaling
  #         ax2 = axes[1, idx]
  #         sns.histplot(all_data_scaled[feat], kde=True, ax=ax2, color='darkgreen', bins=30)
  #         ax2.set_title(f'{feat} - Après RobustScaler\nMean: {all_data_scaled[feat].mean():.2f}, Std: {all_data_scaled[feat].std():.2f}',
  #                     fontsize=10)

  # axes[0, 2].remove() if len(sample_features) < 3 else None
  # axes[1, 2].remove() if len(sample_features) < 3 else None

  # plt.tight_layout()
  # plt.show()

  return X_scaled, scaler

"""### **2.9 Sélection de features** (plus utilisé)"""

print("\n" + "=" * 80)
print("2.9 SÉLECTION DE FEATURES (ANALYSE DE L'IMPORTANCE)")
print("=" * 80)

# Utiliser uniquement les données d'entraînement pour l'analyse
X_train_processed = all_data_scaled.iloc[:ntrain].copy()

# --- FIX: One-hot encode HouseAgeBin column if it exists and is non-numeric ---
if 'HouseAgeBin' in X_train_processed.columns:
    if X_train_processed['HouseAgeBin'].dtype == 'object' or pd.api.types.is_categorical_dtype(X_train_processed['HouseAgeBin']):
        print(f"  ✓ One-hot encoding 'HouseAgeBin' column...")
        X_train_processed = pd.get_dummies(X_train_processed, columns=['HouseAgeBin'], drop_first=True)
# --------------------------------------------------------------------------------

# Final check to ensure all columns are numeric
non_numeric_cols = X_train_processed.select_dtypes(exclude=np.number).columns
if len(non_numeric_cols) > 0:
    print(f" Warning: Non-numeric columns found before MI calculation: {non_numeric_cols.tolist()}")
    # Attempt to convert to numeric, forcing errors for inspection if conversion fails
    for col in non_numeric_cols:
        try:
            X_train_processed[col] = pd.to_numeric(X_train_processed[col])
            print(f"   Converted non-numeric column '{col}' to numeric.")
        except ValueError:
            print(f"  Error: Could not convert column '{col}' to numeric. Dropping it.")
            X_train_processed = X_train_processed.drop(columns=col)

# Calcul de l'information mutuelle
print(f" Calcul de l'information mutuelle avec SalePrice...")
mi_scores = mutual_info_regression(X_train_processed, y_train, random_state=42)
mi_df = pd.DataFrame({
    'Feature': X_train_processed.columns,
    'MI_Score': mi_scores
}).sort_values('MI_Score', ascending=False)

print(f" Top 20 features par information mutuelle:")
display(mi_df.head(20).style.background_gradient(cmap='Greens', subset=['MI_Score']))

# Visualisation
fig, axes = plt.subplots(1, 2, figsize=(18, 8))

# Top features MI
ax1 = axes[0]
top_mi = mi_df.head(15)
colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(top_mi)))
bars = ax1.barh(top_mi['Feature'], top_mi['MI_Score'], color=colors)
ax1.set_xlabel('Information Mutuelle')
ax1.set_title('Top 15 - Importance par Information Mutuelle', fontsize=12, fontweight='bold')
ax1.invert_yaxis()

# Distribution des scores MI
ax2 = axes[1]
ax2.hist(mi_df['MI_Score'], bins=30, color='steelblue', edgecolor='black', alpha=0.7)
ax2.axvline(mi_df['MI_Score'].mean(), color='red', linestyle='--',
           label=f'Moyenne: {mi_df["MI_Score"].mean():.3f}')
ax2.axvline(mi_df['MI_Score'].median(), color='green', linestyle='--',
           label=f'Médiane: {mi_df["MI_Score"].median():.3f}')
ax2.set_xlabel('Score d\'Information Mutuelle')
ax2.set_ylabel('Nombre de features')
ax2.set_title('Distribution des scores d\'importance', fontsize=12, fontweight='bold')
ax2.legend()

plt.tight_layout()
plt.savefig('2_05_feature_importance.png', dpi=150, bbox_inches='tight')
plt.show()

# Features à faible importance (candidats à la suppression)
low_importance_threshold = 0.001
low_importance_features = mi_df[mi_df['MI_Score'] < low_importance_threshold]['Feature'].tolist()

print(f"\n{'='*60}")
print(f"FEATURES À FAIBLE IMPORTANCE (< {low_importance_threshold}):")
print(f"  • Nombre: {len(low_importance_features)}")
print(f"  • Liste: {low_importance_features[:10]}...")
print(f" Ces features peuvent être supprimées pour réduire la dimensionnalité")
print(f"    (conservées pour l'instant)")
print(f"{'='*60}")

"""### **Débogage avant à la fin du prétraitement**"""

class DebugTransformer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        print("\n--- DEBUG: Après prétraitement ---")
        print("Dimension:", X.shape)
        num_nans = X.isnull().sum().sum()
        print("Nombre de NaNs:", num_nans)
        if num_nans > 0:
            print("Colonnes avec NAN et leurs types:")
            nan_cols_df = X.isnull().sum()
            nan_cols_df = nan_cols_df[nan_cols_df > 0]
            for col in nan_cols_df.index:
                print(f"  - {col}: Dtype={X[col].dtype}, NaNs={nan_cols_df[col]}")
        else:
            print("Aucune valeur manquante trouvée.")
        print("----------------------------------\n")
        return X

"""##**3. Modélisation**"""

new_numeric_features = numeric_features
# Colonnes supprimées
drop_cols = [
    '1stFlrSF', '2ndFlrSF', 'TotalBsmtSF',
    'OpenPorchSF', '3SsnPorch', 'EnclosedPorch',
    'YearBuilt', 'YearRemodAdd', 'YrSold',
    'GarageYrBlt', 'Fireplaces'
]
# Actualiser la liste des features numeriques
new_numeric_features = list(set(new_numeric_features) - set(drop_cols))

from sklearn.pipeline import Pipeline
from sklearn.linear_model import ElasticNetCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_selector as selector

# Prétraitement
pipe = Pipeline([
    ('missing', MissingValuesHandler(none_features=none_features, zero_features=zero_features, group_impute=group_impute, mode_features=mode_features)),
    ('anomaly', AnomalyCorrector()),
    ('features', FeatureEngineer()),
    ('ordinal', OrdinalEncoderCustom()),
    ('debug', DebugTransformer()),
    ('preprocess', ColumnTransformer([
        ('num', Pipeline([
            ('scaler', StandardScaler())
        ]), selector(dtype_exclude=object)),
        ('nom', OneHotEncoder(
            handle_unknown='ignore',
            sparse_output=False
        ), selector(dtype_include=object))
    ],
   remainder='passthrough' ))
])

pipe.fit(X_train, y_train)

X_train_transformed = pipe.transform(X_train)
X_test_transformed = pipe.transform(X_test)



model = ElasticNetCV(cv=5, random_state=42)
model.fit(X_train_transformed, y_train)

y_train_pred = model.predict(X_train_transformed)
y_test_pred = model.predict(X_test_transformed)

# Evaluation sur le train
mse_train = mean_squared_error(y_train, y_train_pred)
r2 = r2_score(y_train, y_train_pred)

# Evaluation sur le test
mse_test = mean_squared_error(y_test, y_test_pred)
r2_test = r2_score(y_test, y_test_pred)

# Affichage
print(f"MSE train: {mse_train:.2f}")
print(f"R2 train: {r2:.2f}")
print(f"MSE test: {mse_test:.2f}")
print(f"R2 test: {r2_test:.2f}")

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet, BayesianRidge, HuberRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import RandomizedSearchCV, cross_val_score, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import time

# Transformation log pour la cible
y_train_log = np.log1p(y_train)
y_test_log = np.log1p(y_test)

# Fonction d'évaluation
def evaluate_model(model, X_train, X_test, y_train, y_test, model_name, use_log=False):
    """Évalue un modèle et retourne les métriques"""
    start_time = time.time()

    # Entraînement
    model.fit(X_train, y_train)

    # Prédictions
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)

    # Si on utilise log, retransformer
    if use_log:
        y_pred_train = np.expm1(y_pred_train)
        y_pred_test = np.expm1(y_pred_test)
        y_train_eval = np.expm1(y_train)
        y_test_eval = np.expm1(y_test)
    else:
        y_train_eval = y_train
        y_test_eval = y_test

    # Calcul des métriques
    train_rmse = np.sqrt(mean_squared_error(y_train_eval, y_pred_train))
    test_rmse = np.sqrt(mean_squared_error(y_test_eval, y_pred_test))
    train_mae = mean_absolute_error(y_train_eval, y_pred_train)
    test_mae = mean_absolute_error(y_test_eval, y_pred_test)
    train_r2 = r2_score(y_train_eval, y_pred_train)
    test_r2 = r2_score(y_test_eval, y_pred_test)

    elapsed_time = time.time() - start_time

    results = {
        'Model': model_name,
        'Train_RMSE': train_rmse,
        'Test_RMSE': test_rmse,
        'Train_MAE': train_mae,
        'Test_MAE': test_mae,
        'Train_R2': train_r2,
        'Test_R2': test_r2,
        'Time_sec': elapsed_time
    }

    return results, y_pred_test

print("Imports et fonction prêts!")

# ============================================================
# 3. MODÉLISATION - Phase 1: Baseline Models (sans tuning)
# ============================================================

print("="*70)
print("PHASE 3: MODÉLISATION - Modèles Baseline")
print("="*70)

baseline_models = {
    'Ridge': Ridge(alpha=1.0),
    'Lasso': Lasso(alpha=1.0, max_iter=10000),
    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=10000),
    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'ExtraTrees': ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1),
    'DecisionTree': DecisionTreeRegressor(random_state=42),
    'KNN': KNeighborsRegressor(n_neighbors=5),
    'SVR': SVR(kernel='rbf', C=1.0),
    'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),
    'BayesianRidge': BayesianRidge(),
    'HuberRegressor': HuberRegressor(max_iter=10000)
}

baseline_results = []

print("\nÉvaluation des modèles baseline avec transformation log...\n")

for name, model in baseline_models.items():
    try:
        print(f"Évaluation de {name}...")
        results, _ = evaluate_model(model, X_train_transformed, X_test_transformed, y_train_log, y_test_log, name, use_log=True)
        baseline_results.append(results)
        print(f"  ✓ Test RMSE: ${results['Test_RMSE']:,.2f} | R²: {results['Test_R2']:.4f} | Temps: {results['Time_sec']:.2f}s")
    except Exception as e:
        print(f"  ✗ Erreur avec {name}: {str(e)}")

# Créer DataFrame des résultats
baseline_df = pd.DataFrame(baseline_results)
baseline_df = baseline_df.sort_values('Test_RMSE')

print("\n" + "="*70)
print("RÉSULTATS BASELINE (triés par Test RMSE)")
print("="*70)
print(baseline_df[['Model', 'Test_RMSE', 'Test_R2', 'Time_sec']].to_string(index=False))

import matplotlib.pyplot as plt
import seaborn as sns

class ModelVisualizer:
    def __init__(self, baseline_df, optimized_df):
        self.baseline_df = baseline_df.copy()
        self.optimized_df = optimized_df.copy()

        # Harmonisation des noms
        self.baseline_df = self.baseline_df.set_index('Model')
        self.optimized_df = self.optimized_df.set_index('Model')

        # ***FIX: Remove '_Optimized' suffix from optimized_df index for comparison***
        self.optimized_df.index = self.optimized_df.index.str.replace('_Optimized', '', regex=False)

    def plot_baseline_vs_optimized_rmse(self):
        common_models = self.baseline_df.index.intersection(self.optimized_df.index)

        comparison = pd.DataFrame({
            'Baseline RMSE': self.baseline_df.loc[common_models, 'Test_RMSE'],
            'Optimized RMSE': self.optimized_df.loc[common_models, 'Test_RMSE']
        })

        comparison = comparison.sort_values('Baseline RMSE')

        plt.figure(figsize=(10, 6))
        comparison.plot(kind='barh')
        plt.xlabel("RMSE")
        plt.title("Baseline vs Optimized – RMSE Comparison")
        plt.gca().invert_yaxis()
        plt.grid(axis='x', linestyle='--', alpha=0.6)
        plt.tight_layout()
        plt.show()

    def plot_rmse_gain(self):
        common_models = self.baseline_df.index.intersection(self.optimized_df.index)

        gain = (
            self.baseline_df.loc[common_models, 'Test_RMSE']
            - self.optimized_df.loc[common_models, 'Test_RMSE']
        )

        gain = gain.sort_values()

        plt.figure(figsize=(8, 5))
        gain.plot(kind='barh', color='green')
        plt.xlabel("Gain de RMSE (Baseline - Optimized)")
        plt.title("Amélioration après optimisation")
        plt.grid(axis='x', linestyle='--', alpha=0.6)
        plt.tight_layout()
        plt.show()

    def plot_final_ranking(self):
        ranking = self.optimized_df.sort_values('Test_RMSE')

        plt.figure(figsize=(8, 5))
        sns.barplot(
            x=ranking['Test_RMSE'],
            y=ranking.index,
            orient='h'
        )
        plt.xlabel("Test RMSE")
        plt.ylabel("Modèle")
        plt.title("Classement final des modèles optimisés")
        plt.grid(axis='x', linestyle='--', alpha=0.6)
        plt.tight_layout()
        plt.show()

    def summary_table(self):
        # Ensure the optimized_df index matches baseline_df for joining
        # The renaming is now done in __init__
        summary = self.baseline_df[['Test_RMSE', 'Test_R2']].join(
            self.optimized_df[['Test_RMSE', 'Test_R2']],
            lsuffix='_Baseline',
            rsuffix='_Optimized'
        )

        summary['RMSE_Gain'] = (
            summary['Test_RMSE_Baseline']
            - summary['Test_RMSE_Optimized']
        )

        return summary.sort_values('Test_RMSE_Optimized')

"""##**4. Optimisation des hyperparamètres**"""

from sklearn.model_selection import RandomizedSearchCV

class ModelOptimizer:
    def __init__(
        self,
        model,
        param_grid,
        model_name,
        scoring='neg_root_mean_squared_error',
        cv=5,
        n_iter=50,
        random_state=42,
        n_jobs=-1
    ):
        self.model = model
        self.param_grid = param_grid
        self.model_name = model_name
        self.scoring = scoring
        self.cv = cv
        self.n_iter = n_iter
        self.random_state = random_state
        self.n_jobs = n_jobs

        self.search = None
        self.best_model = None
        self.best_params = None
        self.best_cv_score = None
        self.results = None

    def optimize(self, X_train, y_train):
        self.search = RandomizedSearchCV(
            estimator=self.model,
            param_distributions=self.param_grid,
            n_iter=self.n_iter,
            cv=self.cv,
            scoring=self.scoring,
            random_state=self.random_state,
            n_jobs=self.n_jobs,
            verbose=0
        )

        self.search.fit(X_train, y_train)

        self.best_model = self.search.best_estimator_
        self.best_params = self.search.best_params_
        self.best_cv_score = -self.search.best_score_

        print(f"\n✓ {self.model_name}")
        print(f"  Meilleurs paramètres : {self.best_params}")
        print(f"  Meilleur CV RMSE     : ${self.best_cv_score:,.2f}")

        return self.best_model

    def evaluate(
        self,
        X_train,
        X_test,
        y_train,
        y_test,
        use_log=False
    ):
        self.results, _ = evaluate_model(
            self.best_model,
            X_train,
            X_test,
            y_train,
            y_test,
            f"{self.model_name}_Optimized",
            use_log=use_log
        )

        print(
            f"  ✓ Test RMSE: ${self.results['Test_RMSE']:,.2f} "
            f"| R²: {self.results['Test_R2']:.4f}"
        )

        return self.results

param_grid_br = {
    'alpha_1': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2],
    'alpha_2': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2],
    'lambda_1': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2],
    'lambda_2': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]
}

br_optimizer = ModelOptimizer(
    model=BayesianRidge(),
    param_grid=param_grid_br,
    model_name="BayesianRidge"
)

best_br = br_optimizer.optimize(
    X_train_transformed,
    y_train_log
)

results_br = br_optimizer.evaluate(
    X_train_transformed,
    X_test_transformed,
    y_train_log,
    y_test_log,
    use_log=True
)

optimized_results.append(results_br)
best_models['BayesianRidge'] = best_br

TOP_K = 4          # nombre de meilleurs modèles à optimiser
N_ITER = 50        # itérations RandomizedSearch
CV = 5

param_grids = {
    'Ridge': {'alpha': np.logspace(-3, 3, 50)},

    'Lasso': {'alpha': np.logspace(-4, 1, 50)},

    'ElasticNet': {
        'alpha': np.logspace(-4, 1, 30),
        'l1_ratio': np.linspace(0.1, 0.9, 9)
    },

    'RandomForest': {
        'n_estimators': [200, 400, 600],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5, 10]
    },

    'GradientBoosting': {
        'n_estimators': [100, 200, 300],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [3, 4, 5]
    },

    'ExtraTrees': {
        'n_estimators': [200, 400, 600],
        'max_depth': [None, 10, 20]
    },

    'DecisionTree': {
        'max_depth': [None, 5, 10, 20],
        'min_samples_split': [2, 5, 10]
    },

    'KNN': {
        'n_neighbors': list(range(3, 25, 2)),
        'weights': ['uniform', 'distance']
    },

    'SVR': {
        'C': np.logspace(-2, 2, 20),
        'epsilon': [0.01, 0.1, 0.2]
    },

    'AdaBoost': {
        'n_estimators': [100, 200, 300],
        'learning_rate': [0.01, 0.05, 0.1]
    },

    'BayesianRidge': {
        'alpha_1': np.logspace(-6, -2, 10),
        'alpha_2': np.logspace(-6, -2, 10),
        'lambda_1': np.logspace(-6, -2, 10),
        'lambda_2': np.logspace(-6, -2, 10)
    },

    'HuberRegressor': {
        'epsilon': [1.1, 1.35, 1.75],
        'alpha': np.logspace(-4, 1, 20)
    }
}

top_models = (
    baseline_df
    .sort_values('Test_RMSE')
    .head(TOP_K)['Model']
    .tolist()
)

print("\nModèles automatiquement sélectionnés pour optimisation :")
print(top_models)

optimized_results = []
best_models = {}

print("\n" + "="*70)
print("PHASE 4 : OPTIMISATION AUTOMATIQUE DES MEILLEURS MODÈLES")
print("="*70)

for model_name in top_models:

    # Sécurité : modèle et grille doivent exister
    if model_name not in baseline_models or model_name not in param_grids:
        print(f"⏭️  {model_name} ignoré (pas de grille définie)")
        continue

    print("\n" + "-"*60)
    print(f"Optimisation automatique de {model_name}")
    print("-"*60)

    try:
        optimizer = ModelOptimizer(
            model=baseline_models[model_name],
            param_grid=param_grids[model_name],
            model_name=model_name,
            n_iter=N_ITER,
            cv=CV
        )

        best_model = optimizer.optimize(
            X_train_transformed,
            y_train_log
        )

        results = optimizer.evaluate(
            X_train_transformed,
            X_test_transformed,
            y_train_log,
            y_test_log,
            use_log=True
        )

        optimized_results.append(results)
        best_models[model_name] = best_model

    except Exception as e:
        print(f"❌ Échec optimisation {model_name} : {e}")

optimized_df = pd.DataFrame(optimized_results).sort_values('Test_RMSE')

print("\n" + "="*70)
print("RÉSULTATS FINAUX — MODÈLES OPTIMISÉS")
print("="*70)

print(
    optimized_df[
        ['Model', 'Test_RMSE', 'Test_R2', 'Time_sec']
    ].to_string(index=False)
)

visualizer = ModelVisualizer(
    baseline_df=baseline_df,
    optimized_df=optimized_df
)

# 1️⃣ Comparaison baseline vs optimisé
visualizer.plot_baseline_vs_optimized_rmse()

# 2️⃣ Gain de performance
visualizer.plot_rmse_gain()

# 3️⃣ Classement final
visualizer.plot_final_ranking()

# 4️⃣ Tableau récapitulatif
summary_df = visualizer.summary_table()
print(summary_df.to_string())

"""##**5. Evaluation du modèle final**

##**6. Sauvegarde du modèle et conclusion**

***Fin du notebook !!!***
"""